<?xml version="1.0" encoding="utf-8"?>
<schedule>
  <conference>
    <title>Distribits 2024</title>
    <subtitle>Technologies for distributed data management</subtitle>
    <venue>Haus der Universität</venue>
    <city>Düsseldorf</city>
    <start>2024-04-04</start>
    <end>2024-04-06</end>
    <time_zone_name>Europe/Berlin</time_zone_name>
  </conference>

  <day index="1" date="2024-04-04">
    <room guid="7f2a480b-3baf-4794-a86a-d9ddabbfc261" name="Foyer">

      <event guid="eea2ea76-d773-4b47-b155-81206ec8b9cb">
        <start>08:30</start>
        <duration>00:30</duration>
        <title>Coffee</title>
        <description>Coffee time</description>
      </event>

      <event guid="204e5eba-5c2b-4535-8049-7920e8b06969">
        <start>12:00</start>
        <duration>01:30</duration>
        <title>Lunch (self-organized, outside venue)</title>
        <description>Lunch time (self-organized, outside venue)</description>
      </event>

      <event guid="62825836-11da-4f09-bdc3-854d05c78828">
        <start>15:00</start>
        <duration>00:30</duration>
        <title>Coffee</title>
        <description>Coffee time</description>
      </event>

      <event guid="45031b96-64f3-444e-a9e7-9e114226e019">
        <start>17:00</start>
        <duration>03:00</duration>
        <title>Dinner and social (self-organized, outside venue)</title>
        <description>Dinner and social time (self-organized, outside venue)</description>
      </event>

    </room>

    <room guid="2834c9b7-49e8-47f4-97ff-bd5c2cbfbc2d" name="Event Hall">

      <event guid="fdc29246-f652-4898-9a32-e09ad3fd5bd5">
        <start>09:00</start>
        <duration>00:30</duration>
        <title>Welcome and overview</title>
        <abstract>Welcome from the organizers</abstract>
      </event>

      <event guid="58439357-716a-443c-9659-2e941e2fa921">
        <start>09:30</start>
        <duration>00:20</duration>
        <title>"What's in the DataLad sandwich" AKA DataLad "ecosystem"</title>
        <abstract>
		  At the heart of many innovative tools lies a simple spark of
		  necessity. For DataLad, that spark was a father's quest in
		  2013 for an effortless way to access free children's
		  cartoons and movies. What started to scratch a
		  personal itch, has evolved into a grant funded DataLad
		  platform addressing a broad range of data logistics
		  challenges. Utilizing the strengths of git and git-annex,
		  DataLad has not only expanded its capabilities but has also
		  contributed to the enhancement of git-annex features,
		  tailor-made to suit its needs. Through the innovative use of
		  git external protocols and git-annex external special
		  remotes, DataLad offers a seamless experience to users,
		  fetching data with remarkable flexibility.
		  To push the boundaries further, DataLad introduced an
		  "extensions mechanism," enabling the platform to adapt and
		  extend beyond its core functionalities. This modular
		  architecture, while offering unparalleled flexibility, hints
		  at a potential for complexity and fragility.
		  In this presentation, I will take you on a journey through
		  the foundational elements that give DataLad its unique
		  extensibility—spanning git, git-annex, and beyond—with few
		  practical examples that bring these concepts to
		  life. Despite the inherent challenges of a modular system,
		  our dedicated "dev-ops" components, which I will
		  demonstrate, ensure a stable and efficient ecosystem. By
		  developing, testing, and distributing these components,
		  we've crafted not just a tool, but a robust platform ready
		  to tackle the data logistics needs of today and tomorrow.
        </abstract>
        <persons>
          <person>Yaroslav Halchenko</person>
        </persons>
      </event>

      <event guid="0fdf066b-37d3-4901-b2ca-86712253b3ac">
        <start>09:50</start>
        <duration>00:20</duration>
        <title>"git annex is complete, right?"</title>
        <abstract>
          My father has asked me this question before over the years. So
          has an experienced developer recently. Seeing the same
          question from two such different perspectives got me asking it
          of myself.
          While a new data storage system can always be added to
          git-annex, or a new command be added to improve some use case,
          both of those can also be accomplished without needing changes
          to git-annex, by external remotes and more targeted frontends
          such as DataLad.
          So what then is the potential surface area of problem space
          that git-annex may expand to cover? Do diminishing returns and
          complexity make such expansions a good idea? I will explore
          this by considering recent developments in git-annex, and the
          impact of lesser-used features.
        </abstract>
        <persons>
          <person>Joey Hess</person>
        </persons>
      </event>

      <event guid="2f3f7aaf-7352-40ab-b0f0-075d294d7be9">
        <start>10:10</start>
        <duration>00:20</duration>
        <title>Git annex recipes</title>
        <abstract>
          I have come up with many recipes over the years for scaling
          git-annex repositories in terms of large numbers of keys,
          large file sizes, and increased transfer efficiency.
          I have working examples that I use internally that I can
          demonstrate. (1) Second-order keys; using metadata to describe
          keys that can be derived from other keys. I primarily used
          this to help with the problem of too many keys referencing
          small files. This is building off of the work of others, but I
          believe I have made useful improvements, and I would like to
          polish it up and share it.
          One very early example is here:
          https://github.com/unqueued/repo.macintoshgarden.org-fileset/
          For now, I stripped out all but the location data from the
          git-annex branch.  Files smaller than 50M are contained in
          second-order keys (8b0b0af8-5e76-449c-b0ae-766fcac0bc58). The
          other uuids are for standard backends, including a Google
          Drive account which has very strict limits on requests, and it
          would have been very difficult to process over 10k keys
          directly.  There are also other cases where keys can be
          reliably reproduced from other keys.
          (2) Differential storage with git-annex using bup (or borg). I
          built of off posts on the forums from years ago, and came up
          with some really useful workflows for combining the benefits
          of git-annex location tracking and branching with differential
          compression. I have scripts used for automation, and some
          example repos and case studies. For example, I have a repo
          which contains file indexes that are over 60GiB, but only
          consume about 6GiB, using bup packfiles. I can benefit from
          differential compression over different time ranges, like per
          year, or for the entire history, while minimizing storage
          usage. I will publish a working example in the next few weeks,
          but I have only used it internally for years.
        </abstract>
        <persons>
          <person>Timothy Sanders</person>
        </persons>
      </event>

      <event guid="bdb82e56-291a-429d-af67-1c60ce41d44d">
        <start>10:30</start>
        <duration>00:20</duration>
        <title>DataLad beyond Git, connecting to the rest of the world</title>
        <abstract>
          DataLad has been built on Git and git-annex as foundational pillars.
          However, the vast majority of data infrastructures are not Git-aware.
          Git-annex can work with a much broader array of services,
          but the need to "keep the Git repo somewhere" imposes undesirable
          technical and procedural complexity on users.
          In this talk I illustrate existing means to take Git-based DataLad
          datasets to places that Git cannot reach on its own.
          Moreover, I introduce ongoing work that aims to enable DataLad
          users to consume non-DataLad resources as native DataLad datasets,
          and non-DataLad users to consume DataLad resources without DataLad,
          git-annex, or even Git.
        </abstract>
        <persons>
          <person>Michael Hanke</person>
        </persons>
      </event>

      <event guid="f6152865-f078-400e-94b1-2a6dc56bd00e">
        <start>10:50</start>
        <duration>00:20</duration>
        <title>OpenNeuro and DataLad</title>
        <abstract>
          A history of OpenNeuro's adoption of DataLad and the evolution
          of DataLad and git-annex support on the platform. In 2017
          OpenNeuro was preparing to launch with the original data
          backend implemented as block storage without git-annex. The
          decision was made to move OpenNeuro to DataLad and a quick
          prototype for this backend service was created and eventually
          brought to production for the public release of OpenNeuro.
          Since 2017 the platform has evolved to support many of the
          unique advantages of DataLad datasets. This talk discusses the
          architecture of OpenNeuro, some of the challenges encountered
          using git-annex as the center of our application’s data model
          in cloud environments, solutions developed, and future work to
          improve upon OpenNeuro’s archival and distribution of DataLad
          datasets.
        </abstract>
        <persons>
          <person>Nell Hardcastle</person>
        </persons>
      </event>

      <event guid="10b5e56d-acd3-4fbf-9441-aaee5387838f">
        <start>11:10</start>
        <duration>00:50</duration>
        <title>Questions and panel discussion</title>
        <abstract>
          Questions and panel discussion
        </abstract>
      </event>

      <event guid="ed7839f6-1ee2-43c4-88e3-11d675700556">
        <start>13:50</start>
        <duration>00:20</duration>
        <title>GIN-Tonic: (trying to) making datalad accessible to non-coders</title>
        <abstract>
          With the GIN-Tonic suite, we work toward a workflow where the
          users only needs to use the browser and some synchronisation
          scripts to manage their datalad repositories. Settings are
          prepared in a template (including submodules) and a browser
          extension (written in Go) create new repositories from that
          template.  I would report on successes and failures of the
          strategy.
        </abstract>
        <persons>
          <person>Julien Colomb</person>
        </persons>
      </event>

      <event guid="7e45daa0-f220-4be3-ab2e-03f966ce0f91">
        <start>14:10</start>
        <duration>00:20</duration>
        <title>Onedata as a Platform: Distributed Repository for Virtualizing Data and Long-term Preservation</title>
        <abstract>
          With the proliferation of digital data, reliable storage, easy
          accessibility, and long-term preservation have become
          paramount. Onedata, a novel platform, emerges as a solution
          for these challenges by enabling a distributed repository
          framework for virtualizing data. This presentation delves into
          how Onedata facilitates seamless data management and ensures
          long-term preservation. By virtualizing data, Onedata
          abstracts the underlying storage infrastructures enabling a
          unified view and easy sharing among different
          stakeholders. Furthermore, its distributed repository nature
          significantly enhances data durability and availability. The
          in-built mechanisms for metadata management and data
          replication ensure that the information remains intact and
          accessible over extended periods. Through a detailed
          exploration of its architecture and functionalities, this
          presentation will highlight how Onedata can be a robust
          platform for modern data management and long-term preservation
          needs, catering to academia, industry, and beyond. The
          insights provided will foster a better understanding of
          leveraging distributed repository platforms in navigating the
          complex landscape of digital data preservation.
        </abstract>
        <persons>
          <person>Łukasz Dutka</person>
        </persons>
      </event>

      <event guid="d5c1b6fb-4d0b-477e-a9ce-63236cb6d0a2">
        <start>14:30</start>
        <duration>00:30</duration>
        <title>Questions and panel discussion</title>
        <abstract>
          Questions and panel discussion
        </abstract>
      </event>

      <event guid="862bbe87-a61b-4dd3-9ef2-fef682383aef">
        <start>15:30</start>
        <duration>00:20</duration>
        <title>Workflow provenance–based scheduling</title>
        <abstract>
          Scientific computing workflows have become increasingly
          complex, often comprising of numerous interdependent tasks
          executed on distributed computing resources. Provenance data,
          or the history of computational processes, provide a vital
          link between data reproducibility and task
          scheduling. Workflows with recorded data provenance can
          seamlessly integrate with separate workflow management
          systems, eliminating the need for inter-system communication.
          In this talk, we introduce a novel tool to perform
          provenance-based workflow scheduling. Our approach leverages
          an abstract graph builder tool designed to create abstract
          graphs representing the high-level structure of
          workflows. These abstract graphs emphasize dependencies and
          data flows, facilitating a better understanding of the
          computational process. Concurrently, we extract concrete
          graphs from workflow provenance data recorded with DataLad
          that reflect the actual execution history.
          The core of our approach lies in comparing the abstract graph
          to concrete graphs produced by separate runs of the workflow
          for a set of input parameters. By computing the difference we
          can pinpoint tasks that remain unexecuted or require
          re-execution due to errors or changes in input data and
          automatically schedule these tasks.  We will outline future
          directions for this research, including potential extensions
          to support system agnostic scheduling, and scalability
          considerations.
        </abstract>
        <persons>
          <person>Pedro Martinez</person>
        </persons>
      </event>

      <event guid="b8e17fd7-e4e9-4d50-8aa0-4df685866d0d">
        <start>15:50</start>
        <duration>00:10</duration>
        <title>Optimisation in Network Engineering: Challenges and Solutions in Research Data Management</title>
        <abstract>
          In the complex realm of network engineering design,
          optimisation methods have been instrumental, using a range of
          components across different systems and scenarios. However,
          this complexity presents a dual challenge: first, managing,
          tracking and combining thousands of optimisation calculations,
          including the specifics of component data, system
          classifications, scenarios considered, and settings
          applied. Second, integrating diverse data from multiple
          sources that do not all reside in one place. Third, the
          possibility of collaboration (in this case with students,
          potentially with more people). Such challenges emphasise the
          need for rigorous research data management. Questions such as
          "which component data was used in which system?" or the
          provenance of component data come to the fore.  To answer
          these questions, DataLad is used to store disparate data,
          models, settings and results in an effective and distributed
          manner. DataLad's provenance reduces the redundancy of storage
          and the effort required for publication, while increasing
          confidence in the results. This is done in the context of a
          research project, but the same questions arise for the
          industrial application of what has been researched.
        </abstract>
        <persons>
          <person>Julius Breuer</person>
        </persons>
      </event>

      <event guid="b2ea30d2-aa61-4c74-ac69-8f890cdfa67c">
        <start>16:00</start>
        <duration>00:10</duration>
        <title>fMRI Pipelines on HPC with DataLad and ReproMan</title>
        <abstract>
          In this lightning talk, I will share my experience using
          DataLad, git-annex and ReproMan to run software pipelines on
          hundreds of fMRI datasets on an HPC cluster. Potential
          topics may include: (a) The use of ReproMan to avoid the
          difficulties of using datalad containers-run in parallel on
          an HPC.  (b) How to use DataLad on a scratch filesystem that
          periodically purges files to save space.  (c) A simple
          algorithm I implemented in ReproMan to prevent excess
          runtime due to outliers in parallel jobs.  (d) The pros and
          cons of the YODA-BIDS layout for neuroimaging data.  I hope
          my talk will prompt discussion with those hoping to learn
          more from my experience as well as those who have found
          alternative solutions to similar challenges.
        </abstract>
        <persons>
          <person>Joe Wechsler</person>
        </persons>
      </event>

      <event guid="72e1aa3f-f94e-41de-b23e-9bb1da3c2122">
        <start>16:10</start>
        <duration>00:10</duration>
        <title>Reproducibility vs. computational efficiency on HPC systems</title>
        <abstract>
          HPC systems have particular hard- and software configurations that
          introduce specific challenges for the implementation of reproducible
          data processing workflows. The DataLad based 'FAIRly big workflow'
          allows for a separation of the compute environment from the
          processing pipeline enabling automatic reproducibility over systems.
          Yet, the sheer size of RAM and CPUs on HPC systems will allow for
          different ways to optimize compute jobs in contrast to compute
          clusters and certainly the average workstation/laptop. In this talk,
          I discuss general differences between HCP and more standard compute
          environments regarding necessary choices for the setup of processing
          pipelines to be reproducible. Among the main factors are the
          availability of RAM, local storage, inodes and wall clock time.
        </abstract>
        <persons>
          <person>Felix Hoffstaedter</person>
        </persons>
      </event>

      <event guid="309a849f-5802-444b-9712-4e8c6614ee61">
        <start>16:20</start>
        <duration>00:40</duration>
        <title>Questions and panel discussion</title>
        <abstract>
          Questions and panel discussion
        </abstract>
      </event>
    </room>
  </day>

  <day index="2" date="2024-04-05">
    <room guid="7f2a480b-3baf-4794-a86a-d9ddabbfc261" name="Foyer">

      <event guid="21380a3f-758a-4d64-88f3-c2fa6ea5eb2a">
        <start>08:30</start>
        <duration>00:30</duration>
        <title>Coffee</title>
        <description>
          Coffee time!
        </description>
      </event>

      <event guid="d815e458-5b41-4f68-b70c-b33b800de40f">
        <start>12:00</start>
        <duration>01:30</duration>
        <title>Lunch (self-organized, outside venue)</title>
        <description>Lunch time (self-organized, outside venue)</description>
      </event>

      <event guid="786aa8b8-ffe9-4e00-8ec5-80ffa1a33ea3">
        <start>15:10</start>
        <duration>00:30</duration>
        <title>Coffee</title>
        <description>
          Coffee time!
        </description>
      </event>

      <event guid="fee8cb07-1c0b-4164-b40b-b1d6668bd1e1">
        <start>17:00</start>
        <duration>01:00</duration>
        <title>Dinner and social (self-organized, outside venue)</title>
        <description>
          Dinner and social time (self-organized, outside venue)
        </description>
      </event>

    </room>

    <room guid="2834c9b7-49e8-47f4-97ff-bd5c2cbfbc2d" name="Event Hall">

      <event guid="dffaf288-3cb2-4fc8-adac-7ccc3cd84ef8">
        <start>09:00</start>
        <duration>00:15</duration>
        <title>Welcome and overview</title>
        <abstract>
          Welcome and overview: day 2.
        </abstract>
      </event>

      <event guid="3c0d2631-55ae-4b46-a549-1803b516c406">
        <start>09:15</start>
        <duration>00:20</duration>
        <title>Neuroscientific data management using DataLad</title>
        <abstract>
          Robust data management from raw data to result publication
          is necessary to make scientific research more widely
          reusable. This remains a challenge, particularly in projects
          that involve a variety of subcomponents and large data. In
          this talk, I provide a user perspective on using DataLad
          procedures for structuring, managing, and sharing complex
          cognitive neuroscience projects. By showcasing example
          multimodal neuroimaging projects that include e.g.,
          electroencephalography (EEG), functional magnetic resonance
          imaging (fMRI), and behavioral data, I will highlight
          workflows that are uniquely enabled by the distributed
          nature of DataLad. Based on my experiences, I will also
          indicate remaining roadblocks I perceive to widespread
          adoption.
        </abstract>
        <persons>
          <person>Julian Kosciessa</person>
        </persons>
      </event>

    <event guid="1ee16c1f-da1b-4035-a71d-6ad4fb2cfc58">
      <start>09:35</start>
      <duration>00:20</duration>
      <title>Staying in Control of your Scientific Data with Git Annex</title>
      <abstract>
        Scientific experiments can produce a lot of data, often very
        different in kind and scattered across devices and even remote
        locations. Keeping all of this in check is not a simple task
        and failure to do so can easily cause data loss due to
        accidental deletion or hardware failure (think cheap SD cards
        in measurement devices at remote locations). Git Annex can
        help with synchronisation, catalogisation, versioning and
        archival of data as well as collaboration.
      </abstract>
      <persons>
        <person>Yann Büchau</person>
      </persons>
    </event>

    <event guid="b2b0097c-afed-4373-a124-69240ab2ccf7">
      <start>09:55</start>
      <duration>00:20</duration>
      <title>Questions and panel discussion</title>
      <abstract>
        Questions and panel discussion
      </abstract>
    </event>

    <event guid="3bfa442d-5c64-48bd-924a-7657f9c0ab8a">
      <start>10:15</start>
      <duration>00:20</duration>
      <title>Fusion of Multiple Open Source Applications for Data Management Workflows in Psychology and Neuroscience</title>
      <abstract>
        Finding a compromise between researchers’ needs, their skills
        in data management, data access restrictions, and limited
        funding for RDM is a complex but highly relevant and timely
        challenge. At the University of Marburg this challenge is
        taken on by the team of the “Data Hub”. The team consists of
        several people with different responsibilities, backgrounds,
        and affiliations such as project management staff, scientific
        staff, data stewards, data scientists, technical
        administrative staff, located in Marburg and Gießen. The Data
        Hub is funded by The Adaptive Mind (TAM) and supported by the
        information infrastructure project (NOWA) of the SFB135, which
        are consortia in the fields of psychology and neuroscience,
        with over 50 involved PIs, based in several locations in the
        federal state of Hesse, Germany.
        Although the research data in the two consortia are restricted
        to the fields of psychology and neuroscience, a major
        challenge is the need to harmonize heterogeneous data. The
        data encompass research data from different modalities such as
        behavior, eye tracking, EEG and neuroimaging as well as code
        for experiments and analysis in various programming
        languages. Therefore, the data management workflow needs to be
        applicable to heterogeneous in- and output data, different
        project sizes, and numbers of researchers
        involved. Furthermore, tools need to be able to integrate
        those heterogeneous data by utilizing a harmonizing standard
        in the field (here: BIDS). To increase the reproducibility of
        research findings, an integration of version control and
        provenance tracking (here: DataLad) should be available.
        For this, the team must have an understanding at which point
        to include the researchers: How much background knowledge
        about software do they have and how much do they really need?
        Which functions of the software are necessary and which ones
        can be skipped because they’ll never apply to the researchers’
        work? Do they need a lot of hands-on practice or is the
        concept enough?
        In our presentation, we will first introduce the Data Hub of
        the University of Marburg and its technical architecture. We
        will then present the data management tools utilized in the
        Data Hub, i.e., DataLad, GIN, GitLab, JupyterHub, and BIDS. We
        will specifically focus on how these tools are interconnected,
        i.e., the research data management workflow of the Data
        Hub. Then, we will outline the challenges for both the
        researchers as well as the Data Stewards regarding training,
        support and maintenance of the services.
      </abstract>
      <persons>
        <person>Julia-Katharina Pfarr</person>
      </persons>
    </event>

    <event guid="7ba85db5-acd0-49ed-8981-e01556937cad">
      <start>10:35</start>
      <duration>00:20</duration>
      <title>Balancing Efficiency and Standardization for a Microscopic Image Repository on an HPC System</title>
      <abstract>
        Understanding the human brain is one of the greatest
        challenges of modern science. In order to study its complex
        structural and functional organization, data from different
        modalities and resolutions must be linked together. This
        requires scalable and reproducible workflows ranging from the
        extraction of multimodal data from different repositories to
        AI-driven analysis and visualization [1]. One fundamental
        challenge therein is to store and organize big image datasets
        in appropriate repositories. Here we address the case of
        building a repository of high-resolution microscopy scans for
        whole human brain sections in the order of multiple Petabytes
        [1]. Since data duplication is prohibitive for such volumes,
        images need to be stored in a way that follows community
        standards, supports provenance tracking, and meets performance
        requirements of high-throughput ingestion, highly parallel
        processing on HPC systems, as well as ad-hoc random access for
        interactive visualization.
        To digitize an entire human brain, high-throughput scanners
        need to capture over 7000 histological brain sections. During
        this process, a scanner acquires a z-stack, which consists of
        30 TIFF images per tissue section, each representing a
        different focus level. The images are automatically
        transferred from the scanner to a gateway server, where they
        are pre-organised into subfolders per brain section for
        detailed automated quality control (QC). Once a z-stack passes
        QC, it is transferred to the parallel file system (GPFS) on
        the supercomputer via NFS-mount. For one human brain, this
        results in 7000 folders with about 2 PByte of image data in
        about 20K files in total. From there, the data are accessed
        simultaneously by different applications and pipelines with
        their very heterogeneous requirements. HPC analyses based on
        Deep Learning such as cell segmentation or brain mapping rely
        on fast random access and parallel I/O to stream image patches
        efficiently to GPUs. Remote visualization and annotation on
        the other hand requires exposure of the data through an HTTP
        service on a VM, with access to higher capacity storage to
        serve different data at the same time. These demands can be
        covered by multi-tier HPC storage, which provides dedicated
        partitions. The High Performance Storage Tier offers low
        latency and high bandwidth for analysis, while the Extended
        Capacity Storage Tier is capacity-optimized with a lower
        latency, meeting the needs for visualization. Exposing the
        data on different tiers requires controlled staging and
        unstaging. We organize the image data folders via DataLad
        datasets, which allows well defined staging across these
        partitions for different applications, ensures that all data
        is tracked and versioned from distributed storage throughout
        the workflow, and enables provenance tracking. To reduce the
        number of files in one DataLad repository, each section folder
        has been designed as a subdataset of a superdataset that
        contains all section folders.
        The current approach to managing data has two
        deficiencies. Firstly, the TIFF format is not optimized for
        HPC usage due to the lack of parallel I/O support, resulting
        in data duplication due to conversion to HDF5. Secondly, the
        current data organization is not compatible with upcoming
        community standards, complicating collaborative
        efforts. Therefore, standardization of the file format and
        folder structure is a major objective for the near future. The
        widely accepted community standard for organizing neuroscience
        data is the Brain Imaging Data Structure (BIDS). Its extension
        for microscopy proposes splitting the data into subjects and
        samples, while using either (OME-)TIFF or OME-ZARR as a file
        format. Particularly, the NGFF file format OME-ZARR appears to
        be the suitable choice for the workflow described, as it is
        more performant on HPC and cloud compatible as opposed to
        TIFF. However, restructuring the current data layout is a
        complex task. Adopting the BIDS standard results in large
        amounts of inodes and files because (1) multiple folders and
        sidecar files are created and (2) OME-ZARR files are comprised
        of many small files. DataLad annex undergoes expansion with
        the increase in the number of files leading to high inode
        usage and reduced performance. An effective solution to this
        problem may involve the optimization of the size of DataLad
        subdatasets. However, the key consideration is that GPFS file
        systems enforce a limit on the number of inodes, which cannot
        be surpassed. This raises the following questions: How can
        usage of inodes be minimized while adhering to BIDS and
        utilizing DataLad?  Should performant file formats with
        minimal inode usage, such as ZARR v3 or HDF5, be incorporated
        into the BIDS standard? What is a good balance for DataLad
        subdataset sizes? Discussions with the community may provide
        valuable perspectives for advancing this issue.
        [1] Amunts K, Lippert T. Brain research challenges
        supercomputing. Science 374, 1054-1055
        (2021). DOI:10.1126/science.abl8519
      </abstract>
      <persons>
        <person>Julia Thönnißen</person>
      </persons>
    </event>

    <event guid="53309795-003f-4a13-8c61-dcb42c13c1ba">
      <start>10:55</start>
      <duration>00:20</duration>
      <title>DataLad-Registry: Bringing Benefits of Centrality to DataLad</title>
      <abstract>
        DataLad-Registry is a service that maintains up-to-date information on over ten
        thousand datasets, with the collection expanding as more datasets are added.
        This talk will explore how DataLad-Registry automatically registers datasets from
        the internet, extracts metadata from them, and keeps these datasets and their
        corresponding metadata up-to-date. We'll showcase the datasets and metadata
        types currently available within DataLad-Registry and demonstrate the service's
        search capability. Additionally, we'll provide an overview of the API and reveal
        the underlying service components of DataLad-Registry. The presentation will
        conclude with a discussion on ongoing and future developments, inviting audience
        input to shape the future of DataLad-Registry.
      </abstract>
      <persons>
        <person>Isaac To</person>
        <person>Yaroslav Halchenko</person>
      </persons>
    </event>

    <event guid="75f3bdf7-674e-448d-a08a-1eb878d144fc">
      <start>11:15</start>
      <duration>00:45</duration>
      <title>Questions and panel discussion</title>
      <abstract>
        Questions and panel discussion
      </abstract>
    </event>

    <event guid="4fc2a76d-13be-41fc-bc09-cfcb170ad4ab">
      <start>13:50</start>
      <duration>00:40</duration>
      <title>Reproducible and replicable data science in the presence of patient confidentiality concerns by utilizing git-annex and the Data Science Orchestrator</title>
      <abstract>
        Health-related data for patients is among the most sensitive
        data when it comes to data privacy concerns. Data science
        projects in the medical domain must thus pass a very high bar
        before allowing data researchers access to potentially
        personally identifiable data, or pseudonymized patient data
        that carries an inherent risk of depseudonymization.
        In the project "Data Science Orchestrator", we propose an
        organizational framework for ethically chaperoning and
        risk-managing such projects while they are under way, and a
        software stack that helps in this task. At the same time this
        software stack will provide an audit trail across the project
        that is verifyable even by external scientists without access
        to the raw data, while keeping the option for future
        reproducibility studies and replicability studies open. This
        is achieved by utilizing git-annex and datalad in a novel way
        to provide partial data blinding.
        Because collecting study-relevant data is often a time- and
        labor-intensive undertaking in the medical domain, many
        projects are undertaken by associations that span multiple
        hospitals, administrative domains, and often even multiple
        states. Therefore the "Data Science Orchestrator" project also
        implements distributed data science computations, which allow
        to honor these existing administrative boundaries by means of
        a federated access model, all while keeping the most sensitive
        data in-house and exclusively in a tightly controlled
        computation environment.
        This work was sponsored by Deutsche Zentren für
        Gesundheitsforschung (DZG) and BMBF.
      </abstract>
      <persons>
        <person>Markus Katharina Brechtel</person>
        <person>Philipp Kaluza</person>
      </persons>
    </event>

    <event guid="f465a39d-4ed4-4609-80e0-8373e2c28739">
      <start>14:30</start>
      <duration>00:20</duration>
      <title>A Tour of Magit</title>
      <abstract>
        Magit is an Emacs interface to Git.  Through it, you can drive Git
        operations, even advanced ones, by typing short key sequences.  This
        talk will show Magit in action.  I will give a general overview and
        then highlight features for preparing and refining a series of
        commits.
      </abstract>
      <persons>
        <person>Kyle Meyer</person>
      </persons>
    </event>

    <event guid="44622511-c5c2-4ca8-9978-fa1a153b8f80">
      <start>14:50</start>
      <duration>00:20</duration>
      <title>Questions and panel discussion</title>
      <abstract>
        Questions and panel discussion
      </abstract>
      <persons>
        <person></person>
      </persons>
    </event>

    <event guid="a81918df-a057-48b1-b016-ce31f59b02a3">
      <start>15:40</start>
      <duration>00:40</duration>
      <title>Distributed Metadata and Data with Dataverse</title>
      <abstract>
        Dataverse is open source research data repository software that has
        supported distributed metadata for a long time and is increasingly
        supporting distributed data. Learn about the Open Archives Initiative
        Protocol for Metadata Harvesting (OAI-PMH) and file "stores" within
        Dataverse that can be hosted locally, on S3, Globus, or remote
        locations. We plan to demonstrate a proof of concept for a distributed
        storage configuration in Jülich DATA and the use of Dataverse APIs to
        manage data with Python.
      </abstract>
      <persons>
        <person>Philip Durbin</person>
        <person>Jan Range</person>
        <person>Oliver Bertuch</person>
      </persons>
    </event>

    <event guid="42070d09-8bc1-459f-a24a-decd1382e301">
      <start>16:20</start>
      <duration>00:40</duration>
      <title>Unconference</title>
      <abstract>
        Unconference slot
      </abstract>
      <persons>
        <person></person>
      </persons>
    </event>

    </room>
  </day>

  <day index="3" date="2024-04-06">
    <room guid="df1b1641-4574-4317-8ae5-d43d2292a81c" name="Seminar room 1, 3rd floor">
      <event guid="a15745bd-fa3b-4b84-afb1-ff3854b7bd09">
        <start>08:30</start>
        <duration>00:30</duration>
        <title>Coffee</title>
        <description>
          Coffee time!
        </description>
      </event>

      <event guid="4ca40acb-9bf2-4bc0-b79a-01f7d3e2ad5e">
        <start>09:00</start>
        <duration>00:30</duration>
        <title>Kick off / Pitches</title>
        <abstract>
        </abstract>
      </event>

      <event guid="1217ead7-89d0-4f0e-b37d-dc00f9ed9d46">
        <start>09:30</start>
        <duration>02:30</duration>
        <title>Hacking</title>
        <abstract>
        </abstract>
      </event>

      <event guid="632c2aca-bcc9-4b6b-8f9a-619bcf1aa79c">
        <start>12:00</start>
        <duration>01:30</duration>
        <title>Lunch (self-organized, outside venue)</title>
        <abstract>
        </abstract>
      </event>

      <event guid="36b06323-76b8-44c7-8675-360dc7eee574">
        <start>13:30</start>
        <duration>01:00</duration>
        <title>Hacking</title>
        <abstract>
        </abstract>
      </event>

      <event guid="66b743bb-4d97-42aa-ab16-131e6a8b3856">
        <start>14:30</start>
        <duration>00:15</duration>
        <title>Coffee</title>
        <description>
          Coffee time!
        </description>
      </event>

      <event guid="f768b81f-3d50-4497-9d96-235c9609d8f4">
        <start>14:45</start>
        <duration>01:45</duration>
        <title>Hacking</title>
        <abstract>
        </abstract>
      </event>

      <event guid="c66498f0-d0ca-44ad-9c11-7ab1a6905089">
        <start>16:30</start>
        <duration>00:30</duration>
        <title>Wrap-up</title>
        <abstract>
        </abstract>
      </event>

      <event guid="10e182e5-9fb8-4f2c-998e-4d583f8c07d9">
        <start>17:00</start>
        <duration>02:00</duration>
        <title>Dinner and social (self-organized, outside venue)</title>
        <description>
          Dinner and social time (self-organized, outside venue)
        </description>
      </event>

    </room>
  </day>

</schedule>
